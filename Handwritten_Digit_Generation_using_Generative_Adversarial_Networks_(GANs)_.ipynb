{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOW8CMQfCgDppXSJTN5paUJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FaiazS/Handwritten-Digit-Generation-using-Generative-Adversarial-Networks-GANs-/blob/main/Handwritten_Digit_Generation_using_Generative_Adversarial_Networks_(GANs)_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H606aFLlhqBV"
      },
      "outputs": [],
      "source": [
        "#Load Libraries\n",
        "\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras import models, layers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading and preprocessing MNIST Dataset\n",
        "\n",
        "def load_data():\n",
        "\n",
        "  (x_train, _), (_, _) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "  x_train = (x_train.astype(np.float32) - 127.5) / 127.5 #Scaling down / Normalizing input to [-1, 1]\n",
        "\n",
        "  x_train = np.expand_dims(x_train, axis = -1) #Adding Channel Dimension\n",
        "\n",
        "  return tf.data.Dataset.from_tensor_slices(x_train).shuffle(60000).batch(128).prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "-qCkx17sicBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Building the Generative Adversarial Network\n",
        "\n",
        "def Generator_Model():\n",
        "\n",
        "  model = tf.keras.Sequential([layers.Dense(7 * 7 * 256, use_bias = False, input_shape =(100, )),\n",
        "\n",
        "                               layers.BatchNormalization(),\n",
        "\n",
        "                               layers.ReLU(),\n",
        "\n",
        "                               layers.Reshape((7, 7, 256)),\n",
        "\n",
        "                               layers.Conv2DTranspose(128, (5 ,5), strides = (1, 1), padding = 'same', use_bias = False),\n",
        "\n",
        "                               layers.BatchNormalization(),\n",
        "\n",
        "                               layers.ReLU(),\n",
        "\n",
        "                               layers.Conv2DTranspose(24, (5, 5), strides = (2, 2), padding = 'same', use_bias = False),\n",
        "\n",
        "                               layers.BatchNormalization(),\n",
        "\n",
        "                               layers.ReLU(),\n",
        "\n",
        "                               layers.Conv2DTranspose(1, (5 ,5), strides = (2, 2), padding = 'same' , activation = 'tanh')\n",
        "\n",
        "                               ])\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "def Discriminator_Model():\n",
        "\n",
        "      model = tf.keras.Sequential([layers.Conv2D(64, (5, 5), strides = (1, 1), padding = 'same', input_shape = [28, 28, 1]),\n",
        "\n",
        "                                   layers.BatchNormalization(),\n",
        "\n",
        "                                   layers.LeakyReLU(alpha = 0.2),\n",
        "\n",
        "                                   layers.Dropout(0.3),\n",
        "\n",
        "                                   layers.Conv2D(128, (5, 5), strides = (2, 2), padding = 'same'),\n",
        "\n",
        "                                   layers.LeakyReLU(alpha = 0.2),\n",
        "\n",
        "                                   layers.Dropout(0.3),\n",
        "\n",
        "                                   layers.Flatten(),\n",
        "\n",
        "                                   layers.Dense(1, activation = 'sigmoid')\n",
        "\n",
        "                                   ])\n",
        "\n",
        "      return model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "N9IymOQxku3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Preprocessing and Compiling\n",
        "\n",
        "def get_optimizers():\n",
        "\n",
        "   return tf.keras.optimizers.Adam(1e-4), tf.keras.optimizers.Adam(1e-4)\n",
        "\n",
        "generator_model = Generator_Model()\n",
        "\n",
        "discriminator_model = Discriminator_Model()\n",
        "\n",
        "loss_function = tf.keras.losses.BinaryCrossentropy(from_logits = True)\n",
        "\n",
        "\n",
        "generator_optimizer, discriminator_optimizer = get_optimizers()"
      ],
      "metadata": {
        "id": "dOXyVkVKrRr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Discriminator wants real images to be classified as 1 and fake images as 0,\n",
        "#On the other hand, Generator wants fake images to be classified as 1.\n",
        "\n",
        "#Compiling to a TensorFlow Graph for Speed.\n",
        "\n",
        "@tf.function\n",
        "\n",
        "def training_step(images):\n",
        "\n",
        "  noise = tf.random.normal((128, 100))  #Defining a batch of 128 noise vectors each of size 100 dimension\n",
        "\n",
        "  with tf.GradientTape() as generator_tape, tf.GradientTape() as discriminator_tape:\n",
        "\n",
        "    generated_images = generator_model(noise, training = True) # Generator takes the random noise as input produces fake images resembling real MNIST Digits.\n",
        "\n",
        "    real_image_output = discriminator_model(images, training = True) # Discriminator predicts whether real MNIST images are real.(Should be close to 1).\n",
        "\n",
        "    fake_image_output = discriminator_model(generated_images, training = True) #Discriminator predicts whether the generated(fake) images are real.(Should be close to 0).\n",
        "\n",
        "    #tf.ones_like(fake_image_output) defines a tensor of ones, representing real labels\n",
        "\n",
        "    #loss_function(Binary Cross Entropy) measures how close fake_image_output is to 1.\n",
        "\n",
        "    #Low generator loss means the Generator Model is successfully fooling the Discriminator Model.\n",
        "\n",
        "    #High generator loss means the Generator Model is not Performing well.\n",
        "\n",
        "    generator_loss = loss_function(tf.ones_like(fake_image_output), fake_image_output)\n",
        "\n",
        "\n",
        "     #How well the discriminator model classfies real images as real(Close to 1). + How well the discriminator model classifies fake images as fake(Close to 0).\n",
        "\n",
        "    discriminator_loss = loss_function(tf.ones_like(real_image_output), real_image_output) + loss_function(tf.zeros_like(fake_image_output), fake_image_output)\n",
        "\n",
        "\n",
        "    generator_gradient = generator_tape.gradient(generator_loss, generator_model.trainable_variables) #Computes the gradient for the parameters of the Generator Model.\n",
        "\n",
        "    discriminator_gradient = discriminator_tape.gradient(discriminator_loss, discriminator_model.trainable_variables)  #Computes the gradient for the parameters of the Discriminator Model.\n",
        "\n",
        "    #Updating the Generator Model's weights\n",
        "\n",
        "    generator_optimizer.apply_gradients(zip (generator_gradient, generator_model.trainable_variables))\n",
        "\n",
        "\n",
        "    #Updating the Discriminator Model's weights\n",
        "\n",
        "    discriminator_optimizer.apply_gradients(zip (discriminator_gradient, discriminator_model.trainable_variables))\n",
        "\n",
        "    return generator_loss, discriminator_loss\n"
      ],
      "metadata": {
        "id": "c6z3mbi63unG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training both the Generator Model and Discriminator Model\n",
        "\n",
        "def train_generator_and_discriminator_models(dataset, epochs = 27):\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "\n",
        "    for batch in dataset:\n",
        "\n",
        "      gen_loss, disc_loss = training_step(batch)\n",
        "\n",
        "def generate_and_show_image():\n",
        "\n",
        "  noise = tf.random.normal((16, 100))\n",
        "\n",
        "  images = generator_model(noise, training = False)\n",
        "\n",
        "  images = (images + 1) / 2.0  #Rescaling to [0,1]\n",
        "\n",
        "  figure, axis = plt.subplots(4, 4, figsize = (7, 7))\n",
        "\n",
        "  for i, ax in enumerate(axis.flat):\n",
        "\n",
        "    ax.imshow(images[i, :, :, 0], cmap = 'gray')\n",
        "\n",
        "    ax.axis('off')\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "#Loading data and training the both Models\n",
        "\n",
        "mnist_data = load_data()\n",
        "\n",
        "train_generator_and_discriminator_models(mnist_data, epochs = 27)\n",
        "\n",
        "generate_and_show_image()\n"
      ],
      "metadata": {
        "id": "Dyl7K2jJInhZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}